{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "050c6046",
   "metadata": {},
   "source": [
    "### Part a): Analytical warm-up\n",
    "\n",
    "When using our gradient machinery from project 1, we will need the expressions for the cost/loss functions and their respective\n",
    "gradients. The functions whose gradients we need are:\n",
    "1. The mean-squared error (MSE) with and without the $L_1$ and $L_2$ norms (regression problems)\n",
    "\n",
    "2. The binary cross entropy (aka log loss)  for binary classification problems with and without $L_1$ and $L_2$ norms\n",
    "\n",
    "3. The multiclass cross entropy cost/loss function (aka Softmax cross entropy or just Softmax loss function)\n",
    "\n",
    "Set up these three cost/loss functions and their respective derivatives and explain the various terms. In this project you will however only use the MSE and the Softmax  cross entropy.\n",
    "\n",
    "We will test three activation functions for our neural network setup, these are the \n",
    "1. The Sigmoid (aka **logit**) function,\n",
    "\n",
    "2. the RELU function and\n",
    "\n",
    "3. the Leaky RELU function\n",
    "\n",
    "Set up their expressions and their first derivatives.\n",
    "You may consult the lecture notes (with codes and more) from week 42 at <https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/week42.html>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYS-STK3155",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
